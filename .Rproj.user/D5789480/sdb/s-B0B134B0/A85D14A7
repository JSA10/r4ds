{
    "collab_server" : "",
    "contents" : "#r4ds chapter 9 wrangle\n#\n#9.1 Tibbles\n#tibbles are an updated version of data frames, with a few tweaks that iron\n# out old bugs from base R versions that are circa 20 years old and work better\n# in tidyverse\n\nlibrary(tidyverse)\n#coerce data frames into tibbles\nas_tibble(iris)\n\n#You can create a new tibble from individual vectors with tibble()\n#tibble() will automatically recycle inputs of length 1, and allows you to\n#refer to variables that you just created, as shown below.\ntibble(x = 1:5, y = 1, z = x ^ 2 + y)\n#> # A tibble: 5 × 3\n#>       x     y     z\n#>   <int> <dbl> <dbl>\n#> 1     1     1     2\n#> 2     2     1     5\n#> 3     3     1    10\n#> 4     4     1    17\n#> 5     5     1    26\n\n\"\"\"\nIf you’re already familiar with data.frame(), note that tibble() does much\nless: it never changes the type of the inputs (e.g. it never converts strings\nto factors!), it never changes the names of variables, and it never creates\nrow names.\n\nIt’s possible for a tibble to have column names that are not valid R variable\nnames, aka non-syntactic names. For example, they might not start with a letter,\nor they might contain unusual characters like a space. To refer to these\nvariables, you need to surround them with backticks, `:\n\"\"\"\n\ntb <- tibble(\n    `:)` = \"smile\",\n    ` ` = \"space\",\n    `2000` = \"number\"\n)\ntb\n#> # A tibble: 1 × 3\n#>    `:)`   ` ` `2000`\n#>   <chr> <chr>  <chr>\n#> 1 smile space number\n\n\"\"\"\nYou’ll also need the backticks when working with these variables in other\npackages, like ggplot2, dplyr, and tidyr.\n\"\"\"\n\n#TRIBBLES\n\"\"\"\nAnother way to create a tibble is with tribble(), short for transposed tibble.\ntribble() is customised for data entry in code: column headings are defined by\nformulas (i.e. they start with ~), and entries are separated by commas. This\nmakes it possible to lay out small amounts of data in easy to read form.\n\"\"\"\ntribble(\n    ~x, ~y, ~z,\n    #--|--|----\n    \"a\", 2, 3.6,\n    \"b\", 1, 8.5\n)\n\n#> # A tibble: 2 × 3\n#>       x     y     z\n#>   <chr> <dbl> <dbl>\n#> 1     a     2   3.6\n#> 2     b     1   8.5\n\n#I often add a comment (the line starting with #), to make it really clear\n#where the header is.\n\n\n#tibbles vs. data frames\n# printing and subsetting are main differences\n\n#printing\n#printing tibbles by name shows only first 10 rows and columns that fit on page\n# = easier to work with big data\n# also each column displays data type - feature borrowed by str\n\n#when need to display more data\n# 1. use print and control rows and cols with n and width\nnycflights13::flights %>%\n    print(n = 10, width = Inf)\n\n# 2. use options to set default print behaviour\noptions(tibble.print_max = n, tibble.print_min = m)\n#if more than m rows, print only n rows.\n\n#Use options(dplyr.print_min = Inf) to always show all rows.\n#Use options(tibble.width = Inf) to always print all columns, regardless of the\n#width of the screen.\n\n#You can see a complete list of options by looking at the package help with\n#package?tibble.\n\n# 3. A final option is to use RStudio’s built-in data viewer to get a scrollable\n#view of the complete dataset. This is also often useful at the end of a long\n#chain of manipulations.\n\nnycflights13::flights %>%\n    View()\n# remember can use commands like head etc. to control amount displayed using\n# view()\n\n\n#subsetting\n#use same tools as dataframes - $ and [[]] but tibbles are stricter\n# - never do partial matching and will generate a warning if the column doesn't\n# exist\n\n# remember:\n# [[]] matches name or position,\n# $ only matches by name but less typing\n\ndf <- tibble(\n    x = runif(5),\n    y = rnorm(5)\n)\ndf$x\n#> [1] 0.434 0.395 0.548 0.762 0.254\ndf[[\"x\"]]\n#> [1] 0.434 0.395 0.548 0.762 0.254\n# Extract by position\ndf[[1]]\n\n# To use these in a pipe, you’ll need to use the special placeholder .:\ndf %>% .$x\n#> [1] 0.434 0.395 0.548 0.762 0.254\ndf %>% .[[\"x\"]]\n#> [1] 0.434 0.395 0.548 0.762 0.254\n\n\n## 10.4 Interacting with older code\n\"\"\"\nSome older functions don’t work with tibbles. If you encounter one of these\nfunctions, use as.data.frame() to turn a tibble back to a data.frame:\n\"\"\"\nclass(as.data.frame(tb))\n#> [1] \"data.frame\"\n\"\"\"\nThe main reason that some older functions don’t work with tibble is the\n[ function. We don’t use [ much in this book because dplyr::filter() and\ndplyr::select() allow you to solve the same problems with clearer code\n(but you will learn a little about it in vector subsetting). With base R\ndata frames, [ sometimes returns a data frame, and sometimes returns a vector.\nWith tibbles, [ always returns another tibble.\n\"\"\"\n\n#10.5 Exercises\n# come back to if want to practise\n# also if want to go deeper check out: vignette(\"tibble\")\n\n\n## 11 Data import\n\n# using readr package\n\n\"\"\"\n11.2 Getting started\n\nMost of readr’s functions are concerned with turning flat files into data\nframes:\n\nread_csv() reads comma delimited files, read_csv2() reads semicolon separated\nfiles (common in countries where , is used as the decimal place), read_tsv()\nreads tab delimited files, and read_delim() reads in files with any delimiter.\n\nread_fwf() reads fixed width files. You can specify fields either by their\nwidths with fwf_widths() or their position with fwf_positions(). read_table()\nreads a common variation of fixed width files where columns are separated by\nwhite space.\n\nread_log() reads Apache style log files. (But also check out webreadr which\nis built on top of read_log() and provides many more helpful tools.)\n\nThese functions all have similar syntax: once you’ve mastered one, you can use\nthe others with ease. For the rest of this chapter we’ll focus on read_csv().\nNot only are csv files one of the most common forms of data storage, but once\nyou understand read_csv(), you can easily apply your knowledge to all the other\nfunctions in readr.\n\"\"\"\n\n#The first argument to read_csv() is the most important: it’s the path to the\n#file to read.\n\nheights <- read_csv(\"data/heights.csv\")\n\n\"\"\"\nWhen you run read_csv() it prints out a column specification that gives the\nname and type of each column. That’s an important part of readr, which we’ll\ncome back to in parsing a file.\n\"\"\"\n#you can also supply an inline csv file. This is useful for experimenting with\n#readr and for creating reproducible examples to share with others:\n\nread_csv(\"a,b,c\n1,2,3\n4,5,6\")\n#> # A tibble: 2 × 3\n#>       a     b     c\n#>   <int> <int> <int>\n#> 1     1     2     3\n#> 2     4     5     6\n\n#two cases where might not want first line to be columns:\n\n# 1. Some metadata at top of the file\n# skip = n    to skip n lines\n# comment = \"#\"     to drop any lines that start with #\n#\n\nread_csv(\"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\", skip = 2)\n\nread_csv(\"# A comment I want to skip\n  x,y,z\n  1,2,3\", comment = \"#\")\n\n# 2. Data might not have column names\n# use col_names = FALSE\nread_csv(\"1,2,3\\n4,5,6\", col_names = FALSE)\n# NOTE: \"\\n\" is a convenient shortcut for adding a new line. More in strings\n\n# or can pass col_names a character vector with new names\nread_csv(\"1,2,3\\n4,5,6\", col_names = c(\"x\", \"y\", \"z\"))\n\n\n# can identify the values in file to be imported that represent missing values\nread_csv(\"a,b,c\\n1,2,.\", na = \".\")\n#> # A tibble: 1 × 3\n#>       a     b     c\n#>   <int> <int> <chr>\n#> 1     1     2  <NA>\n\n\"\"\"\nThis is all you need to know to read ~75% of CSV files that you’ll encounter in\npractice. You can also easily adapt what you’ve learned to read tab separated\nfiles with read_tsv() and fixed width files with read_fwf().\n\nTo read in more challenging files, you’ll need to learn more about how readr\nparses each column, turning them into R vectors.\n\"\"\"\n\n\"\"\"\n11.2.1 Compared to base R\n\nIf you’ve used R before, you might wonder why we’re not using read.csv(). There\nare a few good reasons to favour readr functions over the base equivalents:\n\nThey are typically much faster (~10x) than their base equivalents. Long running\njobs have a progress bar, so you can see what’s happening. If you’re looking\nfor raw speed, try data.table::fread(). It doesn’t fit quite so well into the\ntidyverse, but it can be quite a bit faster.\n\nThey produce tibbles, they don’t convert character vectors to factors, use row\nnames, or munge the column names. These are common sources of frustration with\nthe base R functions.\n\nThey are more reproducible. Base R functions inherit some behaviour from your\noperating system and environment variables, so import code that works on your\ncomputer might not work on someone else’s.\n\"\"\"\n\n## section on parsing vectors is useful reading for understanding how readr\n## works\n\n\"\"\"\nUsing parsers is mostly a matter of understanding what’s available and how they\ndeal with different types of input. There are eight particularly important\nparsers:\n\nparse_logical() and parse_integer() parse logicals and integers respectively.\nThere’s basically nothing that can go wrong with these parsers so I won’t\ndescribe them here further.\n\nparse_double() is a strict numeric parser, and parse_number() is a flexible\nnumeric parser. These are more complicated than you might expect because\ndifferent parts of the world write numbers in different ways.\n\nparse_character() seems so simple that it shouldn’t be necessary. But one\ncomplication makes it quite important: character encodings.\n\nparse_factor() create factors, the data structure that R uses to represent\ncategorical variables with fixed and known values.\n\nparse_datetime(), parse_date(), and parse_time() allow you to parse various\ndate & time specifications. These are the most complicated because there are\nso many different ways of writing dates.\n\"\"\"\n\n#for now just need basics - use the import section as a reference as you go\n\nread_csv()\n# In order to allocate character types automatically readr uses a strategy of\n# analyzing first 1000 rows and then 'guessing' the col type using certain rules\n# worth knowing as can lead to problems in a couple special examples.\n\n\"\"\"\nThe heuristic tries each of the following types, stopping when it finds a match:\n\nlogical: contains only “F”, “T”, “FALSE”, or “TRUE”.\ninteger: contains only numeric characters (and -).\ndouble: contains only valid doubles (including numbers like 4.5e-5).\nnumber: contains valid doubles with the grouping mark inside.\ntime: matches the default time_format.\ndate: matches the default date_format.\ndate-time: any ISO8601 date.\nIf none of these rules apply, then the column will stay as a vector of strings.\n\"\"\"\n\nwrite_csv(object, \"filename.csv\")\n#NOTE: writing to a csv is good for compatibility of sharing (UTF-8 and\n#date/time in ISO8601 format), but doesn't preserve r formatting and metadata)\n#use rds format for caching / saving interim data that will work on again\n\"\"\"\nwrite_rds() and read_rds() are uniform wrappers around the base functions\nreadRDS() and saveRDS(). These store data in R’s custom binary format called\nRDS\n\"\"\"\nwrite_rds(challenge, \"challenge.rds\")\nread_rds(\"challenge.rds\")\n\n\n#11.6 Other types of data\n\"\"\"\nTo get other types of data into R, we recommend starting with the tidyverse\npackages listed below. They’re certainly not perfect, but they are a good\nplace to start. For rectangular data:\n\nhaven reads SPSS, Stata, and SAS files.\n\nreadxl reads excel files (both .xls and .xlsx).\n\nDBI, along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL\netc) allows you to run SQL queries against a database and return a data frame.\n\nFor hierarchical data: use jsonlite (by Jeroen Ooms) for json, and xml2 for\nXML. Jenny Bryan has some excellent worked examples at\nhttps://jennybc.github.io/purrr-tutorial/examples.html.\n\nFor other file types, try the R data import/export manual and the rio package.\n\"\"\"\n\n\n\n\n### 12. TIDY DATA\n\n#Recap\n\n#There are three interrelated rules which make a dataset tidy:\n\n#1. Each variable must have its own column.\n#2. Each observation must have its own row.\n#3. Each value must have its own cell.\n\n\"\"\"\nThese three rules are interrelated because it’s impossible to only satisfy\ntwo of the three. That interrelationship leads to an even simpler set of\npractical instructions:\n\n*****\nBROADLY THERE ARE TWO JOBS TO DO TO MAKE DATA TIDY:\nPut each dataset in a tibble.\nPut each variable in a column.\n*****\n\n\"\"\"\n\n\n\"\"\"\nWhy ensure that your data is tidy? There are two main advantages:\n\n1. There’s a general advantage to picking one consistent way of storing data. If\nyou have a consistent data structure, it’s easier to learn the tools that work\nwith it because they have an underlying uniformity.\n\n2. There’s a specific advantage to placing variables in columns because it allows\nR’s vectorised nature to shine. As you learned in mutate and summary functions,\nmost built-in R functions work with vectors of values. That makes transforming\ntidy data feel particularly natural.\n\ndplyr, ggplot2, and all the other packages in the tidyverse are designed to\nwork with tidy data.\n\"\"\"\n\n#examples\n\n# Compute rate per 10,000\ntable1 %>%\n    mutate(rate = cases / population * 10000)\n\n# Compute cases per year\ntable1 %>%\n    count(year, wt = cases)\n\n# Visualise changes over time\nlibrary(ggplot2)\nggplot(table1, aes(year, cases)) +\n    geom_line(aes(group = country), colour = \"grey50\") +\n    geom_point(aes(colour = country))\n\n\n\"\"\"\nThe principles of tidy data seem so obvious that you might wonder if you’ll\never encounter a dataset that isn’t tidy. Unfortunately, however, most data\nthat you will encounter will be untidy. There are two main reasons:\n\n1. Most people aren’t familiar with the principles of tidy data, and it’s hard to\nderive them yourself unless you spend a lot of time working with data.\n\n2. Data is often organised to facilitate some use other than analysis. For example,\ndata is often organised to make entry as easy as possible.\n\nThis means for most real analyses, you’ll need to do some tidying. The first\nstep is always to figure out what the variables and observations are. Sometimes\nthis is easy; other times you’ll need to consult with the people who originally\ngenerated the data. The second step is to resolve one of two common problems:\n\nOne variable might be spread across multiple columns.\n\nOne observation might be scattered across multiple rows.\n\nTypically a dataset will only suffer from one of these problems; it’ll only\nsuffer from both if you’re really unlucky! To fix these problems, you’ll need\nthe two most important functions in tidyr: gather() and spread().\n\"\"\"\n\n# GATHER\n# gather() used when you have columns that aren't distinct variables, likely to\n#  be categories within a variable that could be combined. So rather than\n#  having 1999, 2000, 2001 as columns they can all be 'gathered' into 1 column\n#  called Years\ngather('colnames', key = new_var_name, value = new_value_name)\n#e.g.\ntable4a\n#> # A tibble: 3 × 3\n#>       country `1999` `2000`\n#> *       <chr>  <int>  <int>\n#> 1 Afghanistan    745   2666\n#> 2      Brazil  37737  80488\n#> 3       China 212258 213766\ntable4a %>%\n    gather(`1999`, `2000`, key = \"year\", value = \"cases\")\n#this gathers the columns 1999 and 2000 into two new columns labelled year and\n#cases\n\"\"\"\nHadley version\nTo tidy a dataset like this, we need to gather those columns into a new pair of\nvariables. To describe that operation we need three parameters:\n\n1. The set of columns that represent values, not variables. In this example, those\nare the columns 1999 and 2000.\n\n2.The name of the variable whose values form the column names. I call that the\nkey, and here it is year.\n\n3. The name of the variable whose values are spread over the cells. I call that\nvalue, and here it’s the number of cases.\n\"\"\"\n\n# SPREAD\n# spread() solves the opposite problem, use when an observation is spread\n# across multiple rows\n\n#e.g\ntable2\n#> # A tibble: 12 × 4\n#>       country  year       type     count\n#>         <chr> <int>      <chr>     <int>\n#> 1 Afghanistan  1999      cases       745\n#> 2 Afghanistan  1999 population  19987071\n#> 3 Afghanistan  2000      cases      2666\n#> 4 Afghanistan  2000 population  20595360\n#> 5      Brazil  1999      cases     37737\n#> 6      Brazil  1999 population 172006362\n#> # ... with 6 more rows\n\n\"\"\"\nNeed two parameters:\n\n1. The column that contains variable names, the key column. Here, it’s type.\n\n2. The column that contains values forms multiple variables, the value column.\nHere it’s count.\n\"\"\"\nspread(table2, key = type, value = count)\n#> # A tibble: 6 × 4\n#>       country  year  cases population\n#> *       <chr> <int>  <int>      <int>\n#> 1 Afghanistan  1999    745   19987071\n#> 2 Afghanistan  2000   2666   20595360\n#> 3      Brazil  1999  37737  172006362\n#> 4      Brazil  2000  80488  174504898\n#> 5       China  1999 212258 1272915272\n#> 6       China  2000 213766 1280428583\n\n\n# Separating & Uniting\n\"\"\"\nseparate() pulls apart one column into multiple columns, by splitting wherever\na separator character appears. Take table3:\n\"\"\"\n\ntable3\n#> # A tibble: 6 × 3\n#>       country  year              rate\n#> *       <chr> <int>             <chr>\n#> 1 Afghanistan  1999      745/19987071\n#> 2 Afghanistan  2000     2666/20595360\n#> 3      Brazil  1999   37737/172006362\n#> 4      Brazil  2000   80488/174504898\n#> 5       China  1999 212258/1272915272\n#> 6       China  2000 213766/1280428583\n\n# problem looks very similar to an Excel 'text to columns' job\n\ntable3 %>%\n    separate(rate, into = c(\"cases\", \"population\"))\n# defaults to splitting on any non alpha numeric character it finds\n\n# can specify the separator using sep argument\ntable3 %>%\n    separate(rate, into = c(\"cases\", \"population\"), sep = \"/\")\n# sep = a regular expression (more in strings)\n\n# BE CAREFUL WITH COL FORMAT\n# Separate leaves the column type as it was originally by default. In this case\n# the column was characters, so both cases and population have adopted\n\n# in order to parse to a more appropriate type, use convert argument:\ntable3 %>%\n    separate(rate, into = c(\"cases\", \"population\"), convert = TRUE)\n\n\"\"\"\nYou can also pass a vector of integers to sep. separate() will interpret the\nintegers as positions to split at. Positive values start at 1 on the far-left\nof the strings; negative value start at -1 on the far-right of the strings.\nWhen using integers to separate strings, the length of sep should be one less\nthan the number of names in into.\n\"\"\"\n#e.g. separate the last two digits of each year. This make this data less tidy,\n#but is useful in other cases, as you’ll see in a little bit.\ntable3 %>%\n    separate(year, into = c(\"century\", \"year\"), sep = 2)\n#> # A tibble: 6 × 4\n#>       country century  year              rate\n#> *       <chr>   <chr> <chr>             <chr>\n#> 1 Afghanistan      19    99      745/19987071\n#> 2 Afghanistan      20    00     2666/20595360\n#> 3      Brazil      19    99   37737/172006362\n#> 4      Brazil      20    00   80488/174504898\n#> 5       China      19    99 212258/1272915272\n#> 6       China      20    00 213766/1280428583\n\n## Unite\n\"\"\"\nunite() is the inverse of separate(): it combines multiple columns into a\nsingle column. You’ll need it much less frequently than separate(), but it’s\nstill a useful tool to have in your back pocket.\n\"\"\"\n# Uniting `table5` makes it tidy\n#We can use unite() to rejoin the century and year columns that we created in\n#the last example. That data is saved as tidyr::table5. unite() takes a data\n#frame, the name of the new variable to create, and a set of columns to\n#combine, again specified in dplyr::select() style:\n\ntable5 %>%\n    unite(new, century, year)\n#> # A tibble: 6 × 3\n#>       country   new              rate\n#> *       <chr> <chr>             <chr>\n#> 1 Afghanistan 19_99      745/19987071\n#> 2 Afghanistan 20_00     2666/20595360\n#> 3      Brazil 19_99   37737/172006362\n#> 4      Brazil 20_00   80488/174504898\n#> 5       China 19_99 212258/1272915272\n#> 6       China 20_00 213766/1280428583\n\n#in this case also need sep argument as default is to place an underscore btw\n#columns\n# we don't want any separator so use \"\"\ntable5 %>%\n    unite(new, century, year, sep = \"\")\n\n\n#12.5 Missing values\n\"\"\"\nChanging the representation of a dataset brings up an important subtlety of\nmissing values. Surprisingly, a value can be missing in one of two possible\nways:\n\n1. Explicitly, i.e. flagged with NA.\n2. Implicitly, i.e. simply not present in the data.\n\"\"\"\n#Let’s illustrate this idea with a very simple data set:\nstocks <- tibble(\n        year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),\n        qtr    = c(   1,    2,    3,    4,    2,    3,    4),\n        return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)\n    )\n\"\"\"\nThere are two missing values in this dataset:\n\nThe return for the fourth quarter of 2015 is explicitly missing, because the\ncell where its value should be instead contains NA.\n\nThe return for the first quarter of 2016 is implicitly missing, because it\nsimply does not appear in the dataset.\n\nOne way to think about the difference is with this Zen-like koan: An explicit\nmissing value is the presence of an absence; an implicit missing value is the\nabsence of a presence.\n\"\"\"\n\n# making implicit missing values explicit\n#\n# 1. display in a table format\nstocks %>%\n    spread(year, return)\n#> # A tibble: 4 × 3\n#>     qtr `2015` `2016`\n#> * <dbl>  <dbl>  <dbl>\n#> 1     1   1.88     NA\n#> 2     2   0.59   0.92\n#> 3     3   0.35   0.17\n#> 4     4     NA   2.66\n\n\n#Because these explicit missing values may not be important in other\n#representations of the data, you can set na.rm = TRUE in gather() to turn\n#explicit missing values implicit:\nstocks %>%\n    spread(year, return) %>%\n    gather(year, return, `2015`:`2016`, na.rm = TRUE)\n#> # A tibble: 6 × 3\n#>     qtr  year return\n#> * <dbl> <chr>  <dbl>\n#> 1     1  2015   1.88\n#> 2     2  2015   0.59\n#> 3     3  2015   0.35\n#> 4     2  2016   0.92\n#> 5     3  2016   0.17\n#> 6     4  2016   2.66\n\n\"\"\"\nAnother important tool for making missing values explicit in tidy data is\ncomplete():\n\"\"\"\nstocks %>%\n    complete(year, qtr)\n#> # A tibble: 8 × 3\n#>    year   qtr return\n#>   <dbl> <dbl>  <dbl>\n#> 1  2015     1   1.88\n#> 2  2015     2   0.59\n#> 3  2015     3   0.35\n#> 4  2015     4     NA\n#> 5  2016     1     NA\n#> 6  2016     2   0.92\n#> # ... with 2 more rows\n\n#complete() takes a set of columns, and finds all unique combinations. It then\n#ensures the original dataset contains all those values, filling in explicit\n#NAs where necessary.\n\n\"\"\"\nThere’s one other important tool that you should know for working with missing\nvalues. Sometimes when a data source has primarily been used for data entry,\nmissing values indicate that the previous value should be carried forward:\n\"\"\"\ntreatment <- tribble(\n        ~ person,           ~ treatment, ~response,\n        \"Derrick Whitmore\", 1,           7,\n        NA,                 2,           10,\n        NA,                 3,           9,\n        \"Katherine Burke\",  1,           4\n    )\n\"\"\"\nYou can fill in these missing values with fill(). It takes a set of columns\nwhere you want missing values to be replaced by the most recent non-missing\nvalue (sometimes called last observation carried forward).\n\"\"\"\ntreatment %>%\n    fill(person)\n#> # A tibble: 4 × 3\n#>             person treatment response\n#>              <chr>     <dbl>    <dbl>\n#> 1 Derrick Whitmore         1        7\n#> 2 Derrick Whitmore         2       10\n#> 3 Derrick Whitmore         3        9\n#> 4  Katherine Burke         1        4\n\n\n\n\n## CASE STUDY - pulling it all together\n# WHO data case study a useful worked through example of tidying a dataset\n# final code pipeline\nwho %>%\n    gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %>%\n    mutate(code = stringr::str_replace(code, \"newrel\", \"new_rel\")) %>%\n    separate(code, c(\"new\", \"var\", \"sexage\")) %>%\n    select(-new, -iso2, -iso3) %>%\n    separate(sexage, c(\"sex\", \"age\"), sep = 1)\n\n\"\"\"\n12.7 Non-tidy data\n\nBefore we continue on to other topics, it’s worth talking briefly about non-tidy\ndata. Earlier in the chapter, I used the pejorative term “messy” to refer to\nnon-tidy data. That’s an oversimplification: there are lots of useful and\nwell-founded data structures that are not tidy data. There are two main reasons\nto use other data structures:\n\nAlternative representations may have substantial performance or space\nadvantages.\n\nSpecialised fields have evolved their own conventions for storing data that\nmay be quite different to the conventions of tidy data.\n\nEither of these reasons means you’ll need something other than a tibble (or\ndata frame). If your data does fit naturally into a rectangular structure\ncomposed of observations and variables, I think tidy data should be your\ndefault choice. But there are good reasons to use other structures; tidy data\nis not the only way.\n\nIf you’d like to learn more about non-tidy data, I’d highly recommend this\nthoughtful blog post by Jeff Leek:\nhttp://simplystatistics.org/2016/02/17/non-tidy-data/\n\"\"\"\n\n",
    "created" : 1493133547474.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2462283202",
    "id" : "A85D14A7",
    "lastKnownWriteTime" : 1493659864,
    "last_content_update" : 1493659864858,
    "path" : "~/Documents/Github/r4ds/r4ds_wrangle.r",
    "project_path" : "r4ds_wrangle.r",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}